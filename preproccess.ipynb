{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "from datetime import datetime\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_incidence = pd.read_csv(\"latest-ARI_incidence.csv\",sep=',')\n",
    "ili_incidence = pd.read_csv(\"latest-ILI_incidence.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_incidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive-api.open-meteo.com/v1/archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capitals ARI\n",
    "params_ari = {\n",
    "    \"latitude\": [50.8503, 42.6977, 50.0755, 52.52, 59.437, 40.4168, 48.8566, 47.4979, 54.6872, 49.8153, 56.9496, 44.4268, 46.0569],\n",
    "    \"longitude\": [4.3517, 23.3219, 14.4378, 13.405, 24.7536, -3.7038, 2.3522, 19.0402, 25.2797, 6.1296, 24.1052, 26.1025, 14.5058],\n",
    "    \"hourly\": \"relative_humidity_2m\",\n",
    "    \"timezone\": \"auto\",\n",
    "    \"start_date\": \"2014-10-05\",\n",
    "    \"end_date\": \"2024-10-13\"\n",
    "}\n",
    "country_names_ari = [\n",
    "    \"BE\", \"BG\", \"CZ\", \"DE\", \"EE\",\n",
    "    \"ES\", \"FR\", \"HU\", \"LT\", \"LU\",\n",
    "    \"LV\", \"RO\", \"SI\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_incidence = ari_incidence[ari_incidence['location'].isin(country_names_ari)]\n",
    "ari_incidence.groupby('location').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses = openmeteo.weather_api(url, params=params_ari) \n",
    "all_data_ari = []\n",
    "\n",
    "# Loop through all responses\n",
    "for country, response in zip(country_names_ari, responses):\n",
    "    # Check if the response is valid\n",
    "    if response is None:\n",
    "        print(f\"No data available for {country}.\")\n",
    "        continue\n",
    "\n",
    "    # Process the hourly data\n",
    "    hourly = response.Hourly()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "    # Create a DataFrame for the current location\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "        \"country\": [country] * len(hourly_relative_humidity_2m)  # Add country column\n",
    "    }\n",
    "\n",
    "    all_data_ari.append(pd.DataFrame(data=hourly_data))\n",
    "\n",
    "data_ari_humidity = pd.concat(all_data_ari, ignore_index=True)\n",
    "\n",
    "\n",
    "# Optionally save to CSV\n",
    "data_ari_humidity.to_csv(\"data_humidity_ari.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ari_humidity.groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ari_temp = {\n",
    " \t\"latitude\": [50.8503, 42.6977, 50.0755, 52.52, 59.437, 40.4168, 48.8566, 47.4979, 54.6872, 49.8153, 56.9496, 44.4268, 46.0569],\n",
    "    \"longitude\": [4.3517, 23.3219, 14.4378, 13.405, 24.7536, -3.7038, 2.3522, 19.0402, 25.2797, 6.1296, 24.1052, 26.1025, 14.5058],\n",
    "\t\"daily\": [\"temperature_2m_max\", \"temperature_2m_min\"],\n",
    "\t\"timezone\": \"auto\",\n",
    "\t\"start_date\": \"2014-10-05\",\n",
    "\t\"end_date\": \"2024-10-13\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses = openmeteo.weather_api(url, params=params_ari_temp) \n",
    "all_data_temp_ari = []\n",
    "\n",
    "# Loop through all responses\n",
    "for country, response in zip(country_names_ari, responses):\n",
    "    # Check if the response is valid\n",
    "    if response is None:\n",
    "        print(f\"No data available for {country}.\")\n",
    "        continue\n",
    "\n",
    "    # Process the daily data\n",
    "    daily = response.Daily()\n",
    "    daily_temperature_2m_max = daily.Variables(0).ValuesAsNumpy()\n",
    "    daily_temperature_2m_min = daily.Variables(1).ValuesAsNumpy()\n",
    "\n",
    "    # Create a DataFrame for the current location\n",
    "    daily_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"temperature_2m_max\": daily_temperature_2m_max,\n",
    "        \"temperature_2m_min\": daily_temperature_2m_min,\n",
    "        \"country\": [country] * len(daily_temperature_2m_max)  # Add country column\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame and append to the list\n",
    "    all_data_temp_ari.append(pd.DataFrame(data=daily_data))\n",
    "\n",
    "data_temp_ari = pd.concat(all_data_temp_ari, ignore_index=True)\n",
    "\n",
    "data_temp_ari.to_csv(\"data_temp_ari.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp_ari.groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for ili humidity\n",
    "params_ili = {\n",
    "    \"latitude\": [\n",
    "        47.5162, 50.8503, 50.0755, 55.6761, 59.437,\n",
    "        48.8566, 37.9838, 45.815, 47.4979, 53.3498,\n",
    "        54.6872, 49.8153, 56.9496, 35.8997, 52.3676,\n",
    "        59.9139, 52.2297, 44.4268, 46.0569\n",
    "    ],\n",
    "    \"longitude\": [\n",
    "        14.5501, 4.3517, 14.4378, 12.5683, 24.7536,\n",
    "        2.3522, 23.7275, 15.9819, 19.0402, -6.2603,\n",
    "        25.2797, 6.1296, 24.1052, 14.5146, 4.9041,\n",
    "        10.7522, 21.0122, 26.1025, 14.5058\n",
    "    ],\n",
    "    \"hourly\": \"relative_humidity_2m\",\n",
    "    \"timezone\": \"auto\",\n",
    "    \"start_date\": \"2014-10-05\",\n",
    "    \"end_date\": \"2024-10-13\"\n",
    "}\n",
    "country_names_ili = [\n",
    "    \"AT\",\"BE\",\"CZ\",\"DK\",\n",
    "    \"EE\",\"FR\",\"GR\",\"HR\",\n",
    "    \"HU\",\"IE\",\"LT\",\"LU\",\n",
    "    \"LV\",\"MT\",\"NL\",\"NO\",\n",
    "    \"PL\",\"RO\",\"SI\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili_incidence = ili_incidence[ili_incidence['location'].isin(country_names_ili)]\n",
    "ili_incidence.groupby('location').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "responses = openmeteo.weather_api(url, params=params_ili) \n",
    "all_data_ili = []\n",
    "\n",
    "for country, response in zip(country_names_ili, responses):\n",
    "    # Check if the response is valid\n",
    "    if response is None:\n",
    "        print(f\"No data available for {country}.\")\n",
    "        continue\n",
    "\n",
    "    # Process the hourly data\n",
    "    hourly = response.Hourly()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "    # Create a DataFrame for the current location\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "        \"country\": [country] * len(hourly_relative_humidity_2m)  # Add country column\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame and append to the list\n",
    "    all_data_ili.append(pd.DataFrame(data=hourly_data))\n",
    "\n",
    "data_ili_humidity = pd.concat(all_data_ili, ignore_index=True)\n",
    "\n",
    "data_ili_humidity.to_csv(\"data_humidity_ili .csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "responses = openmeteo.weather_api(url, params=params_ili) \n",
    "all_data_ili = []\n",
    "\n",
    "for country, response in zip(country_names_ili, responses):\n",
    "    if response is None:\n",
    "        print(f\"No data available for {country}.\")\n",
    "        continue\n",
    "\n",
    "    hourly = response.Hourly()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "        \"country\": [country] * len(hourly_relative_humidity_2m)\n",
    "    }\n",
    "\n",
    "    all_data_ili.append(pd.DataFrame(data=hourly_data))\n",
    "    # Wait a bit before the next request to avoid rate limiting\n",
    "    time.sleep(5)  # adjust the delay as needed\n",
    "\n",
    "data_ili_humidity = pd.concat(all_data_ili, ignore_index=True)\n",
    "data_ili_humidity.to_csv(\"data_humidity_ili.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ili_humidity.groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for temp ili\n",
    "params_ili_temp = {\n",
    "    \"latitude\": [\n",
    "        47.5162, 50.8503, 50.0755, 55.6761, 59.437,\n",
    "        48.8566, 37.9838, 45.815, 47.4979, 53.3498,\n",
    "        54.6872, 49.8153, 56.9496, 35.8997, 52.3676,\n",
    "        59.9139, 52.2297, 44.4268, 46.0569\n",
    "    ],\n",
    "    \"longitude\": [\n",
    "        14.5501, 4.3517, 14.4378, 12.5683, 24.7536,\n",
    "        2.3522, 23.7275, 15.9819, 19.0402, -6.2603,\n",
    "        25.2797, 6.1296, 24.1052, 14.5146, 4.9041,\n",
    "        10.7522, 21.0122, 26.1025, 14.5058\n",
    "    ],\n",
    "\t\"daily\": [\"temperature_2m_max\", \"temperature_2m_min\"],\n",
    "\t\"timezone\": \"auto\",\n",
    "\t\"start_date\": \"2014-10-05\",\n",
    "\t\"end_date\": \"2024-10-13\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses = openmeteo.weather_api(url, params=params_ili_temp)  \n",
    "all_data_temp_ili = []\n",
    "\n",
    "for country, response in zip(country_names_ili, responses):\n",
    "    # Check if the response is valid\n",
    "    if response is None:\n",
    "        print(f\"No data available for {country}.\")\n",
    "        continue\n",
    "\n",
    "    # Process the daily data\n",
    "    daily = response.Daily()\n",
    "    daily_temperature_2m_max = daily.Variables(0).ValuesAsNumpy()\n",
    "    daily_temperature_2m_min = daily.Variables(1).ValuesAsNumpy()\n",
    "\n",
    "    # Create a DataFrame for the current location\n",
    "    daily_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"temperature_2m_max\": daily_temperature_2m_max,\n",
    "        \"temperature_2m_min\": daily_temperature_2m_min,\n",
    "        \"country\": [country] * len(daily_temperature_2m_max)  # Add country column\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame and append to the list\n",
    "    all_data_temp_ili.append(pd.DataFrame(data=daily_data))\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "data_temp_ili = pd.concat(all_data_temp_ili, ignore_index=True)\n",
    "\n",
    "# Optionally save to CSV\n",
    "data_temp_ili.to_csv(\"data_temp_ili.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp_ili.groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ari_humidity.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ari_humidity['Fecha'] = data_ari_humidity['date'].dt.strftime('%Y-%m-%d')\n",
    "data_ili_humidity['Fecha'] = data_ili_humidity['date'].dt.strftime('%Y-%m-%d')\n",
    "data_temp_ari['Fecha'] = data_temp_ari['date'].dt.strftime('%Y-%m-%d')\n",
    "data_temp_ili['Fecha'] = data_temp_ili['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean by day for humidity\n",
    "data_ili_hum_by_day = data_ili_humidity[['relative_humidity_2m', 'country', 'Fecha']].groupby(['country', 'Fecha'])['relative_humidity_2m'].mean().reset_index()\n",
    "data_ari_hum_by_day = data_ari_humidity[['relative_humidity_2m', 'country', 'Fecha']].groupby(['country','Fecha'])['relative_humidity_2m'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ari_hum_by_day['Fecha'] = pd.to_datetime(data_ari_hum_by_day['Fecha'])\n",
    "data_ili_hum_by_day['Fecha'] = pd.to_datetime(data_ili_hum_by_day['Fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add variable Week of years\n",
    "data_ari_hum_by_day['week_of_year'] = data_ari_hum_by_day['Fecha'].dt.isocalendar().week.astype('str').str.zfill(2)\n",
    "\n",
    "data_ari_hum_by_day['year'] = data_ari_hum_by_day['Fecha'].dt.strftime('%Y')\n",
    "data_ari_hum_by_day['year_week'] = data_ari_hum_by_day['year'] + '-W'+data_ari_hum_by_day['week_of_year']\n",
    "\n",
    "data_ili_hum_by_day['week_of_year'] = data_ili_hum_by_day['Fecha'].dt.isocalendar().week.astype('str').str.zfill(2)\n",
    "data_ili_hum_by_day['year'] = data_ili_hum_by_day['Fecha'].dt.strftime('%Y')\n",
    "data_ili_hum_by_day['year_week'] = data_ili_hum_by_day['year'] + '-W'+data_ili_hum_by_day['week_of_year']\n",
    "\n",
    "data_temp_ari['week_of_year'] = pd.to_datetime(data_temp_ari['Fecha']).dt.isocalendar().week.astype('str').str.zfill(2)\n",
    "data_temp_ari['year'] = pd.to_datetime(data_temp_ari['Fecha']).dt.strftime('%Y')\n",
    "data_temp_ari['year_week'] = data_temp_ari['year'] + '-W'+data_temp_ari['week_of_year']\n",
    "\n",
    "data_temp_ili['week_of_year'] = pd.to_datetime(data_temp_ili['Fecha']).dt.isocalendar().week.astype('str').str.zfill(2)\n",
    "data_temp_ili['year'] = pd.to_datetime(data_temp_ili['Fecha']).dt.strftime('%Y')\n",
    "data_temp_ili['year_week'] = data_temp_ili['year'] + '-W'+data_temp_ili['week_of_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean by week for humidity\n",
    "data_ari_hum_by_week = data_ari_hum_by_day[['country', 'Fecha', 'relative_humidity_2m','year_week']].groupby(['country', 'year_week'],as_index=False).agg({'relative_humidity_2m':'mean'})\n",
    "data_ili_hum_by_week = data_ili_hum_by_day[['country', 'Fecha', 'relative_humidity_2m','year_week']].groupby(['country', 'year_week'],as_index=False).agg({'relative_humidity_2m':'mean'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean by week for temp\n",
    "\n",
    "data_ari_temp_by_week = data_temp_ari[['temperature_2m_max', 'temperature_2m_min', 'country', 'Fecha','year_week']].groupby(['country', 'year_week'],as_index=False).agg({'temperature_2m_max': 'mean', 'temperature_2m_min': 'mean'})\n",
    "data_ili_temp_by_week = data_temp_ili[['temperature_2m_max', 'temperature_2m_min', 'country', 'Fecha','year_week']].groupby(['country', 'year_week'],as_index=False).agg({'temperature_2m_max': 'mean', 'temperature_2m_min': 'mean'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_incidence = ari_incidence[ari_incidence['location'].isin(country_names_ari)]\n",
    "ili_incidence = ili_incidence[ili_incidence['location'].isin(country_names_ili)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_hum = pd.merge(ari_incidence,data_ari_hum_by_week,left_on=['location','year_week'],\n",
    "                   right_on = ['country','year_week'],how = 'left')\n",
    "ari_hum = ari_hum.drop(columns = ['country'])\n",
    "\n",
    "ari = pd.merge(ari_hum,data_ari_temp_by_week,left_on=['location','year_week'],\n",
    "               right_on = ['country','year_week'],how='left')\n",
    "ari = ari.drop(columns=['country'])\n",
    "ari['truth_date'] = pd.to_datetime(ari['truth_date'])\n",
    "ari.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili_hum = pd.merge(ili_incidence,data_ili_hum_by_week,left_on=['location','year_week'],\n",
    "                   right_on = ['country','year_week'],how = 'left')\n",
    "ili_hum = ili_hum.drop(columns = ['country'])\n",
    "\n",
    "ili = pd.merge(ili_hum,data_ili_temp_by_week,left_on=['location','year_week'],\n",
    "               right_on = ['country','year_week'],how='left')\n",
    "ili = ili.drop(columns=['country'])\n",
    "ili['truth_date'] = pd.to_datetime(ili['truth_date'])\n",
    "ili.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili = ili.sort_values(by='truth_date',ascending=True).reset_index(drop=True)\n",
    "ari = ari.sort_values(by='truth_date',ascending=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandemic from march 2020 to may 2024\n",
    "ili['covid']=np.where((ili['truth_date']>='2020-03-01') & (ili['truth_date']<='2024-05-31'),1,0)\n",
    "ari['covid']=np.where((ari['truth_date']>='2020-03-01') & (ari['truth_date']<='2024-05-31'),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter the country\n",
    "data = ari[ari['location'] == 'LV'].copy()\n",
    "\n",
    "# 2. Set index\n",
    "data.set_index('truth_date', inplace=True)\n",
    "\n",
    "# 3. Detect outliers\n",
    "outlier_dates = data[data['value'] > 5000].index\n",
    "\n",
    "# 4. Replace outliers with NaN\n",
    "data.loc[outlier_dates, 'value'] = np.nan\n",
    "\n",
    "# 5. Compute moving average\n",
    "moving_avg = data['value'].rolling(window=7, center=True, min_periods=1).mean()\n",
    "\n",
    "# 6. Fill NaNs in data using moving average\n",
    "data['value'] = data['value'].fillna(moving_avg)\n",
    "\n",
    "# 7. Write the corrected values back into `ari`\n",
    "ari.loc[(ari['location'] == 'LV') & (ari['truth_date'].isin(data.index)), 'value'] = data['value'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter the country\n",
    "data = ili[ili['location'] == 'LV'].copy()\n",
    "\n",
    "# 2. Set index\n",
    "data.set_index('truth_date', inplace=True)\n",
    "\n",
    "# 3. Detect outliers\n",
    "outlier_dates = data[data['value'] > 5000].index\n",
    "\n",
    "# 4. Replace outliers with NaN\n",
    "data.loc[outlier_dates, 'value'] = np.nan\n",
    "\n",
    "# 5. Compute moving average\n",
    "moving_avg = data['value'].rolling(window=7, center=True, min_periods=1).mean()\n",
    "\n",
    "# 6. Fill NaNs in data using moving average\n",
    "data['value'] = data['value'].fillna(moving_avg)\n",
    "\n",
    "# 7. Write the corrected values back into `ili`\n",
    "ili.loc[(ili['location'] == 'LV') & (ili['truth_date'].isin(data.index)), 'value'] = data['value'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili.to_csv(\"data_ili.csv\",sep=\",\")\n",
    "ari.to_csv(\"data_ari.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ari = ari[\"location\"].unique()\n",
    "name_ili = ili[\"location\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ari:\n",
    "    print(f\"\\n📍 ADF Test for: {i}\")\n",
    "    data = ari[ari['location'] == i]['value'].dropna()\n",
    "\n",
    "    result = adfuller(data, autolag='AIC')\n",
    "    test_stat, p_value, lags, n_obs = result[:4]\n",
    "    crit_values = result[4]\n",
    "\n",
    "    print(f\"Test Statistic      : {test_stat:.4f}\")\n",
    "    print(f\"p-value             : {p_value:.4f}\")\n",
    "    print(f\"# Lags Used         : {lags}\")\n",
    "    print(f\"# Observations Used : {n_obs}\")\n",
    "    \n",
    "    for key, value in crit_values.items():\n",
    "        print(f\"Critical Value ({key}) : {value:.4f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"✅ Likely Stationary (reject H0)\")\n",
    "    else:\n",
    "        print(\"❌ Likely Non-stationary (fail to reject H0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ili:\n",
    "    print(f\"\\n📍 ADF Test for: {i}\")\n",
    "    data = ili[ili['location'] == i]['value'].dropna()\n",
    "\n",
    "    result = adfuller(data, autolag='AIC')\n",
    "    test_stat, p_value, lags, n_obs = result[:4]\n",
    "    crit_values = result[4]\n",
    "\n",
    "    print(f\"Test Statistic      : {test_stat:.4f}\")\n",
    "    print(f\"p-value             : {p_value:.4f}\")\n",
    "    print(f\"# Lags Used         : {lags}\")\n",
    "    print(f\"# Observations Used : {n_obs}\")\n",
    "    \n",
    "    for key, value in crit_values.items():\n",
    "        print(f\"Critical Value ({key}) : {value:.4f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"✅ Likely Stationary (reject H0)\")\n",
    "    else:\n",
    "        print(\"❌ Likely Non-stationary (fail to reject H0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['LU']:\n",
    "    print(f\"Processing location: {i}\")\n",
    "    data = ari[ari['location']==i]\n",
    "    data.set_index('truth_date', inplace=True)\n",
    "\n",
    "    # Decompose series\n",
    "    result = seasonal_decompose(data['value'], model='additive', period=26)\n",
    "    resid = result.resid.dropna()\n",
    "    result.plot()\n",
    "    # Detect outliers in residuals\n",
    "    z_scores = (resid - resid.mean()) / resid.std()\n",
    "    outliers = np.abs(z_scores) > 3\n",
    "    \n",
    "    outlier_dates = resid.index[outliers]\n",
    "    outlier_df = data.loc[outlier_dates, ['value']].copy()\n",
    "    print(outlier_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['LU','MT','LV']:\n",
    "    print(f\"Processing location: {i}\")\n",
    "    data = ili[ili['location']==i]\n",
    "    data.set_index('truth_date', inplace=True)\n",
    "\n",
    "    # Decompose series\n",
    "    result = seasonal_decompose(data['value'], model='additive', period=26)\n",
    "    resid = result.resid.dropna()\n",
    "    result.plot()\n",
    "    # Detect outliers in residuals\n",
    "    z_scores = (resid - resid.mean()) / resid.std()\n",
    "    outliers = np.abs(z_scores) > 3\n",
    "    \n",
    "    outlier_dates = resid.index[outliers]\n",
    "    outlier_df = data.loc[outlier_dates, ['value']].copy()\n",
    "    print(outlier_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['LU']:\n",
    "    \n",
    "    data = ari[ari['location']==i]\n",
    "    data.set_index('truth_date', inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    print(f\"Processing location: {i}\")\n",
    "    plt.plot(data.index, data[\"value\"], color='blue', label='Training Data')\n",
    "    plt.title(f\"Time Series for {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['LU','MT','LV']:\n",
    "    \n",
    "    data = ili[ili['location']==i]\n",
    "    data.set_index('truth_date', inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    print(f\"Processing location: {i}\")\n",
    "    plt.plot(data.index, data[\"value\"], color='blue', label='Training Data')\n",
    "    plt.title(f\"Time Series for {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
