{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6416da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import pmdarima as pm\n",
    "from pmdarima import auto_arima,arima\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# get functions from utils.py\n",
    "from utils import *\n",
    "from joblib import dump\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, LayerNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari = pd.read_csv(\"data_ari.csv\",sep=\",\",dtype={'location':str,'year_week':str,\n",
    "                                                'value':np.float32,'relative_humidity_2m':np.float64,\n",
    "                                                'temperature_2m_max':np.float64,'temperature_2m_min':np.float64},\n",
    "                                                parse_dates=['truth_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7100839",
   "metadata": {},
   "outputs": [],
   "source": [
    "ili = pd.read_csv(\"data_ili.csv\",sep=\",\",dtype={'location':str,'year_week':str,\n",
    "                                                'value':np.float32,'relative_humidity_2m':np.float64,\n",
    "                                                'temperature_2m_max':np.float64,'temperature_2m_min':np.float64},\n",
    "                                                parse_dates=['truth_date'])\n",
    "ili = ili.drop(columns=['Unnamed: 0']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_rnn(full_df, location, date, n_input=16, n_output=4):\n",
    "    # Filter country and set data\n",
    "    data_location = full_df[full_df['location'] == location].copy()\n",
    "    data_location['truth_date'] = pd.to_datetime(data_location['truth_date'])\n",
    "    data_location = data_location.set_index('truth_date')\n",
    "\n",
    "    # features\n",
    "    data = create_features(data_location)\n",
    "    data.drop(columns=['location'], inplace=True)\n",
    "\n",
    "    train = data[data.index <= date].copy()\n",
    "    test = data[data.index > date].copy()\n",
    "\n",
    "    # Scaling \n",
    "    var = ['relative_humidity_2m', 'temperature_2m_max', 'temperature_2m_min',\n",
    "           'lag_value_1', 'lag_humidity_1', 'lag_temp_max_1', 'lag_temp_min_1',\n",
    "           'lag_value_2', 'lag_humidity_2', 'lag_temp_max_2', 'lag_temp_min_2',\n",
    "           'lag_value_3', 'lag_humidity_3', 'lag_temp_max_3', 'lag_temp_min_3',\n",
    "           'lag_value_4', 'lag_humidity_4', 'lag_temp_max_4', 'lag_temp_min_4']\n",
    "    \n",
    "    var2 = ['value', 'week_mas_1', 'week_mas_2', 'week_mas_3']\n",
    "    print(train.corr()['value'].sort_values(ascending=False))\n",
    "    scal = MinMaxScaler()\n",
    "    scal_2 = MinMaxScaler()\n",
    "\n",
    "    train[var] = scal.fit_transform(train[var])\n",
    "    test[var] = scal.transform(test[var])\n",
    "\n",
    "    train[var2] = scal_2.fit_transform(train[var2])\n",
    "    test[var2] = scal_2.transform(test[var2])\n",
    "\n",
    "    # union data to generate sequences\n",
    "    full_scaled = pd.concat([train, test])\n",
    "    full_scaled.drop(columns=['week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    print(full_scaled.columns)\n",
    "    X_all, y_all = generate_sequences(full_scaled, target_col='value', n_input=n_input, n_output=n_output)\n",
    "\n",
    "    # dates\n",
    "    dates = full_scaled.index[n_input + n_output - 1:]\n",
    "\n",
    "    idx = dates.get_indexer([pd.to_datetime(date)])\n",
    "    \n",
    "    split_idx = idx[0]\n",
    "\n",
    "    # Train test split to train model\n",
    "    X_train, y_train = X_all[:split_idx], y_all[:split_idx]\n",
    "    X_test, y_test = X_all[split_idx:], y_all[split_idx:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scal_2, scal, dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_rnn(data):\n",
    "    \"\"\"\n",
    "    Create additional features for the non sequential dataset.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    # Extract year, month, day, weekday, and week from 'truth_date'\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "\n",
    "    week = data['year_week'].str.split('-W').str[1]\n",
    "    data['week'] = week.astype(int)\n",
    "    for h in range(1,53):\n",
    "        data[f'lag_value_{h}'] = data['value'].shift(h)\n",
    "        data[f'lag_humidity_{h}'] = data['relative_humidity_2m'].shift(h)\n",
    "        data[f'lag_temp_max_{h}'] = data['temperature_2m_max'].shift(h)\n",
    "        data[f'lag_temp_min_{h}'] = data['temperature_2m_min'].shift(h)\n",
    "    data = data.dropna()\n",
    "    # Convert cyclical categorical variables to category type\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month']/12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month']/12)\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week']/52)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week']/52)\n",
    "    data = data.drop(columns=['month', 'week','year_week'])\n",
    "    data['week_mas_1'] = data['value'].shift(-1)\n",
    "    data['week_mas_2'] = data['value'].shift(-2)\n",
    "    data['week_mas_3'] = data['value'].shift(-3)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39604e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])\n",
    "mape_ili = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(X_train, y_train, n_features=76, n_input=52, epochs=100, batch_size=32,\n",
    "                     n_output=4, model_name=None, country=None):\n",
    "\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2), input_shape=(n_input, n_features)),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.2)),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Bidirectional(LSTM(32, return_sequences=False, recurrent_dropout=0.2)),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(n_output)  \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "    lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_split=0.1,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=[early_stop, lr_schedule],\n",
    "              verbose=1)\n",
    "\n",
    "    # Save model\n",
    "    dump(model, f\"models_rnn/rnn_model_{country}_{model_name}.joblib\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c11c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_rnn(model,X_test,y_test,n_features=76, n_input=16, n_output=4, model_name=None, country=None, mape=None, scal_2=None,dates=None):\n",
    "\n",
    "    y_test_df = pd.DataFrame(y_test, columns=['value', 'week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    true_real = pd.DataFrame(scal_2.inverse_transform(y_test_df), columns=['value', 'week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    # Rolling predictions\n",
    "    rolling_raw = []\n",
    "    for i in range(len(X_test)):\n",
    "        input_seq = X_test[i].reshape(1, n_input, n_features)\n",
    "        pred_scaled = model.predict(input_seq, verbose=0)\n",
    "        rolling_raw.append(pred_scaled.flatten())\n",
    "\n",
    "    rolling_df = pd.DataFrame(rolling_raw, columns=[f\"prediction_{i+1}_weeks\" for i in range(n_output)])\n",
    "    rolling_df_inverse = pd.DataFrame(scal_2.inverse_transform(rolling_df), columns=[f\"prediction_{i+1}_weeks\" for i in range(n_output)])\n",
    "\n",
    "    results = []\n",
    "    for i in range(n_output):\n",
    "        mae, rmse = eval_metrics(true_real.iloc[:, i], rolling_df_inverse.iloc[:, i])\n",
    "        results.append([country, model_name, f\"{i+1}_week\", mae, rmse])\n",
    "\n",
    "    mape = pd.concat([\n",
    "        mape,\n",
    "        pd.DataFrame(results, columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "    ], ignore_index=True)\n",
    "    rolling_df_inverse.index = dates[-rolling_df_inverse.shape[0]:]\n",
    "    true_real.index = dates[-true_real.shape[0]:] \n",
    "    return rolling_df_inverse, mape, true_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba57a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "HU = ari[ari['location']=='HU']\n",
    "HU.head(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ari = ari.location.unique()\n",
    "name_ili = ili.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb56d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['HU']:\n",
    "    print(f\"Training model for {i}\")\n",
    "    X_train, y_train, X_test, y_test, scal_2, scal, dates = train_test_rnn(ari,i,'2023-05-21', n_input=52, n_output=4)\n",
    "    print(X_train.shape)\n",
    "    model= build_lstm_model(X_train,y_train ,n_features=28, n_input=52, epochs=200, batch_size=32,\n",
    "                 n_output=4, model_name='ARI', country=i)\n",
    "    y_train_df = pd.DataFrame(y_train, columns=['value', 'week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    y_train_df_inv = pd.DataFrame(scal_2.inverse_transform(y_train_df), columns=['value', 'week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    y_train_df_inv.index = dates[:y_train_df_inv.shape[0]]\n",
    "    test_aux, mape_ari, true_real= prediction_rnn(model,X_test,y_test,n_features=28, n_input=52, n_output=4, model_name='ARI', country=i, mape=mape_ari, scal_2=scal_2,dates=dates)\n",
    "    test_aux['value'] = true_real['value']\n",
    "    test_aux.to_csv(f'results_RNN_{i}_ari.csv',index=False)\n",
    "    plot_train_test(y_train_df_inv, test_aux, 'ARI', i,'rnn')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7af486",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari.to_csv('mape_ari_rnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de777eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ili:\n",
    "    print(f\"Training model for {i}\")\n",
    "    X_train, y_train, X_test, y_test, scal_2, scal, dates = train_test_rnn(ili,i,'2023-10-15', n_input=52, n_output=4)\n",
    "    model= build_lstm_model(X_train,y_train ,n_features=28, n_input=52, epochs=200, batch_size=32,\n",
    "                 n_output=4, model_name='ILI', country=i)\n",
    "    y_train_df = pd.DataFrame(y_train, columns=['value', 'week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    y_train_df_inv = pd.DataFrame(scal_2.inverse_transform(y_train_df), columns=['value', 'week_mas_1', 'week_mas_2', 'week_mas_3'])\n",
    "    y_train_df_inv.index = dates[:y_train_df_inv.shape[0]]\n",
    "    test_aux, mape_ili, true_real= prediction_rnn(model,X_test,y_test,n_features=28, n_input=52, n_output=4, model_name='ILI', country=i, mape=mape_ili, scal_2=scal_2,dates=dates)\n",
    "    test_aux['value'] = true_real['value']\n",
    "    test_aux.to_csv(f'results_RNN_{i}_ili.csv',index=False)\n",
    "    plot_train_test(y_train_df_inv, test_aux, 'ILI', i,'rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d011142",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili.to_csv('mape_ili_rnn.csv', index=False,decimal=',',sep=';', float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
