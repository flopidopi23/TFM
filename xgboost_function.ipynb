{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6416da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "# get functions from utils.py\n",
    "from utils import eval_metrics,plot_train_test,train_data_ml\n",
    "from joblib import dump\n",
    "import gc\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import glob\n",
    "import joblib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari = pd.read_csv(\"data_ari.csv\",sep=\",\",dtype={'location':str,'year_week':str,\n",
    "                                                'value':np.float32,'relative_humidity_2m':np.float64,\n",
    "                                                'temperature_2m_max':np.float64,'temperature_2m_min':np.float64},\n",
    "                                                parse_dates=['truth_date'])\n",
    "#ari = ari.drop(columns=['Unnamed: 0']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7100839",
   "metadata": {},
   "outputs": [],
   "source": [
    "ili = pd.read_csv(\"data_ili.csv\",sep=\",\",dtype={'location':str,'year_week':str,\n",
    "                                                'value':np.float32,'relative_humidity_2m':np.float64,\n",
    "                                                'temperature_2m_max':np.float64,'temperature_2m_min':np.float64},\n",
    "                                                parse_dates=['truth_date'])\n",
    "ili = ili.drop(columns=['Unnamed: 0']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5429071",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])\n",
    "mape_ili = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4936d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_function(train,test,country=None,model_name=None,mape=None):\n",
    "    X = train.drop(columns=['value','week_mas_1','week_mas_2','week_mas_3'])\n",
    "    y = train[['value','week_mas_1','week_mas_2','week_mas_3']]\n",
    "\n",
    "    X_test = test.drop(columns=['value','week_mas_1','week_mas_2','week_mas_3'])\n",
    "    \n",
    "\n",
    "    base_model =MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', random_state=2332, n_jobs=-1))\n",
    "\n",
    "    base_model.fit(X, y)\n",
    "\n",
    "    importances = np.array([est.feature_importances_ for est in base_model.estimators_])\n",
    "\n",
    "    mean_importances = importances.mean(axis=0)\n",
    "\n",
    "\n",
    "    selected_mask = mean_importances >= 0.01\n",
    "    selected_features = X.columns[selected_mask].tolist()\n",
    "    X_selected = X[selected_features]\n",
    "    # 5. Tunning parameters\n",
    "    param_grid = {\n",
    "        \"estimator__n_estimators\": [50, 100, 200],\n",
    "        \"estimator__max_depth\": [3, 4, 5, 10],\n",
    "        \"estimator__learning_rate\": np.linspace(0.01, 0.1, 10),\n",
    "        \"estimator__subsample\": [0.8, 1.0],\n",
    "        \"estimator__colsample_bytree\": [0.7, 0.8, 1.0],       # feature subsampling\n",
    "        \"estimator__min_child_weight\": [1, 3, 5],              # min data needed in a child\n",
    "        \"estimator__gamma\": [0, 0.1, 0.3, 0.5],                # min loss reduction to split\n",
    "        \"estimator__reg_alpha\": np.linspace(0, 1, 5),          # L1 regularization\n",
    "        \"estimator__reg_lambda\": np.linspace(0, 1, 5),         # L2 regularization\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        random_state=2332\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_selected, y)\n",
    "\n",
    "    model_final = random_search.best_estimator_\n",
    "    model_final.fit(X_selected, y)\n",
    "    model_final.selected_features_ = selected_features\n",
    "    model_final.country_ = country\n",
    "    model_final.model_name_ = model_name\n",
    "\n",
    "    dump(model_final, f\"models_xgboost/xgboost_model_{country}_{model_name}.joblib\")\n",
    "    test_aux = test.copy()\n",
    "    prediction_columns = [f\"prediction_{h+1}_weeks\" for h in range(4)]\n",
    "    preds = model_final.predict(X_test[selected_features])\n",
    "    test_aux[prediction_columns] = preds\n",
    "    # Evaluate predictions\n",
    "    test_aux = test_aux.dropna()\n",
    "    mae0, rmse0 = eval_metrics(test_aux[\"value\"], test_aux[\"prediction_1_weeks\"])\n",
    "    mae1, rmse1  = eval_metrics(test_aux[\"week_mas_1\"], test_aux[\"prediction_2_weeks\"])\n",
    "    mae2, rmse2  = eval_metrics(test_aux[\"week_mas_2\"], test_aux[\"prediction_3_weeks\"])\n",
    "    mae3, rmse3 = eval_metrics(test_aux[\"week_mas_3\"], test_aux[\"prediction_4_weeks\"])\n",
    "\n",
    "    mape = pd.concat([\n",
    "    mape,\n",
    "    pd.DataFrame([\n",
    "        [country, model_name, \"1_week\", mae0, rmse0],\n",
    "        [country, model_name, \"2_week\", mae1, rmse1],\n",
    "        [country, model_name, \"3_week\", mae2, rmse2],\n",
    "        [country, model_name, \"4_week\", mae3, rmse3]\n",
    "    ], columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "], ignore_index=True)\n",
    "    return model_final, selected_features, test_aux,mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde54c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])\n",
    "mape_ili = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91826943",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ari = ari.location.unique()\n",
    "name_ili = ili.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ari:\n",
    "    print(f\"Processing location: {i}\")\n",
    "    train, test = train_data_ml(ari,i, \"2023-10-13\")\n",
    "    train = train.drop(columns=['location'])\n",
    "    test = test.drop(columns=['location'])\n",
    "    model_final, selected_features, test_aux,mape_ari= xgboost_function(train,test,country =i, model_name='ARI', mape=mape_ari)\n",
    "    test_aux.to_csv(f'resultados/xgboost/results_xgboost_{i}_ari.csv',index=False,sep=';',decimal=',')\n",
    "    plot_train_test(train, test_aux,\"ARI\",i,'XGboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari.to_csv(\"mape_ari_xgboost.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f290dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ili:\n",
    "    print(f\"Processing location: {i}\")\n",
    "    train, test = train_data_ml(ili,i, \"2023-10-13\")\n",
    "    train = train.drop(columns=['location'])\n",
    "    test = test.drop(columns=['location'])\n",
    "    model_final, selected_features, test_aux,mape_ili= xgboost_function(train,test,country =i, model_name='ILI', mape=mape_ili)\n",
    "    test_aux.to_csv(f'resultados/xgboost/results_xgboost_{i}_ili.csv',index=False,sep=';',decimal=',')\n",
    "    plot_train_test(train, test_aux,\"ILI\",i,'XGboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd468205",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili.to_csv(\"mape_ili_xgboost.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for path in glob.glob(\"models_xgboost/xgboost_model_*_*.joblib\"):\n",
    "    model = joblib.load(path)\n",
    "\n",
    "    # Extract country and model name from filename if not stored in model\n",
    "    m = re.search(r\"xgboost_model_(.+?)_(.+?)\\.joblib$\", path)\n",
    "    country = getattr(model, \"country_\", m.group(1) if m else \"UNK\")\n",
    "    model_name = getattr(model, \"model_name_\", m.group(2) if m else \"UNK\")\n",
    "\n",
    "    # Recover selected features\n",
    "    if hasattr(model, \"selected_features_\"):\n",
    "        feats = model.selected_features_\n",
    "    else:\n",
    "        try:\n",
    "            feats = list(model.estimators_[0].feature_names_in_)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"No feature names available for {path}. \"\n",
    "                             \"Please re-save with .selected_features_\")\n",
    "\n",
    "    # Extract feature importances for each horizon\n",
    "    for h_idx, est in enumerate(model.estimators_, start=1):\n",
    "        imps = est.feature_importances_\n",
    "        rows.extend([\n",
    "            {\"country\": country, \"model_name\": model_name,\n",
    "             \"horizon\": h_idx, \"feature\": f, \"importance\": imp}\n",
    "            for f, imp in zip(feats, imps)\n",
    "        ])\n",
    "\n",
    "all_imp = pd.DataFrame(rows)\n",
    "all_imp.to_csv(\"xgboost_feature_importances.csv\", index=False,sep=';',decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9832d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add disease column\n",
    "def extract_disease(name):\n",
    "    name = name.lower()\n",
    "    if \"ari\" in name:\n",
    "        return \"ARI\"\n",
    "    elif \"ili\" in name:\n",
    "        return \"ILI\"\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "all_imp[\"disease\"] = all_imp[\"model_name\"].map(extract_disease)\n",
    "# Compute mean and std importance per feature per disease\n",
    "feat_stats = (\n",
    "    all_imp\n",
    "    .groupby([\"disease\", \"feature\"])[\"importance\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "# Get top 10 features per disease based on mean importance\n",
    "top10 = (\n",
    "    feat_stats\n",
    "    .sort_values([\"disease\", \"mean\"], ascending=[True, False])\n",
    "    .groupby(\"disease\")\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(top10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- ARI ----\n",
    "feat_stats_ari = (\n",
    "    feat_stats[feat_stats[\"disease\"] == \"ARI\"]\n",
    "    .sort_values(\"mean\", ascending=True)\n",
    "    .tail(10)   # top 10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(feat_stats_ari[\"feature\"], feat_stats_ari[\"mean\"], xerr=feat_stats_ari[\"std\"])\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.title(\"Top 10 Gradient Boosting Features – ARI\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"xgboost_feature_importance_ARI.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- ILI ----\n",
    "feat_stats_ili = (\n",
    "    feat_stats[feat_stats[\"disease\"] == \"ILI\"]\n",
    "    .sort_values(\"mean\", ascending=True)\n",
    "    .tail(10)   # top 10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(feat_stats_ili[\"feature\"], feat_stats_ili[\"mean\"], xerr=feat_stats_ili[\"std\"])\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.title(\"Top 10 Gradient Boosting Features – ILI\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"xgboost_feature_importance_ILI.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
