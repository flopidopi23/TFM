{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54583165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import pmdarima as pm\n",
    "from pmdarima import auto_arima,arima\n",
    "import warnings\n",
    "# get functions from utils.py\n",
    "from utils import train_data,eval_metrics,plot_train_test\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import gc\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari = pd.read_csv(\"data_ari.csv\",sep=\",\",dtype={'location':str,'year_week':str,\n",
    "                                                'value':np.float32,'relative_humidity_2m':np.float64,\n",
    "                                                'temperature_2m_max':np.float64,'temperature_2m_min':np.float64},\n",
    "                                                parse_dates=['truth_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ee1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ili = pd.read_csv(\"data_ili.csv\",sep=\",\",dtype={'location':str,'year_week':str,\n",
    "                                                'value':np.float32,'relative_humidity_2m':np.float64,\n",
    "                                                'temperature_2m_max':np.float64,'temperature_2m_min':np.float64},\n",
    "                                                parse_dates=['truth_date'])\n",
    "ili = ili.drop(columns=['Unnamed: 0']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d872dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ari = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])\n",
    "mape_ili = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430af5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_arima_sarima_model(train, test, mape,model,order_model,seasonal_order_model, model_name=\"no_model_def\", country=\"no_country_def\",exogenous_var=None):\n",
    "    test_aux = test.copy()\n",
    "\n",
    "    # Prepare prediction columns\n",
    "    for h in range(4):\n",
    "        test_aux[f\"prediction_{h+1}_weeks\"] = np.nan\n",
    "\n",
    "    # Rolling forecast\n",
    "    for i in range(len(test_aux)):\n",
    "        # Combine train and observed test values so far\n",
    "        train_series = pd.concat([train[\"value\"], test_aux.iloc[:i][\"value\"]])\n",
    "        if exogenous_var is not None:\n",
    "            exog_train = pd.concat([train[exogenous_var], test_aux.iloc[:i][exogenous_var]])\n",
    "            exog_forecast = test_aux.iloc[i:i+4][exogenous_var]\n",
    "        else:\n",
    "            exog_train = None\n",
    "            exog_forecast = None        \n",
    "\n",
    "        # Fit model\n",
    "        model = SARIMAX(train_series, order=order_model, seasonal_order=seasonal_order_model,exog=exog_train)\n",
    "        model_fit = model.fit(disp=False)\n",
    "        \n",
    "        # Forecast 1 to 4 weeks ahead, or less at the end\n",
    "        forecast_steps = min(4, len(test_aux) - i)\n",
    "        if exogenous_var is not None:\n",
    "            forecast = model_fit.forecast(steps=forecast_steps, exog=exog_forecast.iloc[:forecast_steps])\n",
    "        else:\n",
    "            forecast = model_fit.forecast(steps=forecast_steps)\n",
    "        # Save forecasted values\n",
    "        #for h, pred in enumerate(forecast):\n",
    "        #    test_aux.loc[test_aux.index[i + h], f\"prediction_{h+1}_weeks\"] = pred\n",
    "        \n",
    "        for h in range(forecast_steps):\n",
    "            test_aux.loc[test_aux.index[i + h], f\"prediction_{h+1}_weeks\"] = forecast.iloc[h]\n",
    "\n",
    "    # Evaluate predictions\n",
    "    for h in range(4):\n",
    "        shifted = test_aux[\"value\"].shift(-h)\n",
    "        preds = test_aux[f\"prediction_{h+1}_weeks\"]\n",
    "        valid_idx = ~shifted.isna()\n",
    "        y_true = shifted[valid_idx]\n",
    "        y_pred = preds[valid_idx]\n",
    "        resid = y_true - y_pred\n",
    "        residual = y_pred - y_true\n",
    "        test_aux.loc[valid_idx, f\"week_{h+1}_res\"] = resid\n",
    "        mae, rmse = eval_metrics(y_true, y_pred)\n",
    "        mape = pd.concat([\n",
    "            mape,\n",
    "            pd.DataFrame([[country, model_name, f\"{h+1}_week\", mae, rmse]],\n",
    "                         columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "        ], ignore_index=True)\n",
    "        print(mape.tail(4))\n",
    "    return mape, test_aux,residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ili = ili['location'].unique()\n",
    "name_ari = ari['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d91a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = joblib.load(f'models/arima_model_RO_ILI.joblib')\n",
    "order_model = loaded_model.order\n",
    "seasonal_order_model = loaded_model.seasonal_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"\n",
    "    Create additional features for the non sequential dataset.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    # Extract year, month, day, weekday, and week from 'truth_date'\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "\n",
    "    week = data['year_week'].str.split('-W').str[1]\n",
    "    data['week'] = week.astype(int)\n",
    "    for h in range(1,5):\n",
    "        data[f'lag_value_{h}'] = data['week_1_res'].shift(h)\n",
    "        data[f'lag_humidity_{h}'] = data['relative_humidity_2m'].shift(h)\n",
    "        data[f'lag_temp_max_{h}'] = data['temperature_2m_max'].shift(h)\n",
    "        data[f'lag_temp_min_{h}'] = data['temperature_2m_min'].shift(h)\n",
    "    data = data.dropna()\n",
    "    # Convert cyclical categorical variables to category type\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month']/12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month']/12)\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week']/52)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week']/52)\n",
    "    data = data.drop(columns=['month', 'week','year_week'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f91391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train, test, model_name, location,folder):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot actual data\n",
    "    plt.plot(train.index, train[\"value\"], color='blue', label='Training Data')\n",
    "    plt.plot(test.index, test[\"value\"], color='orange', label='Actual Test Data')\n",
    "\n",
    "    # Unified forecast column names\n",
    "    forecast_horizons = {\n",
    "        \"prediction_1_weeks\": (\"1 Week Ahead\", \"green\", 0),\n",
    "        \"prediction_2_weeks\": (\"2 Weeks Ahead\", \"red\", 1),\n",
    "        \"prediction_3_weeks\": (\"3 Weeks Ahead\", \"purple\", 2),\n",
    "        \"prediction_4_weeks\": (\"4 Weeks Ahead\", \"brown\", 3),\n",
    "    }\n",
    "\n",
    "    for col, (label, color, shift_val) in forecast_horizons.items():\n",
    "        if col in test.columns:\n",
    "            # Shift predictions forward by their horizon\n",
    "            plt.plot(test.index, test[col].shift(shift_val), linestyle='--', color=color, label=f'{model_name} {label}')\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f\"{model_name} Forecasting â€“ {location}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    plt.savefig(f\"plots_{folder}/plot_{location}_{model_name}.jpg\", format='jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed764c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_variable_selection_and_hyperparam_tuning(train,test,country =None, model_name=None, mape=None):\n",
    "    X = train.drop(columns=['week_1_res','week_2_res','week_3_res','week_4_res'])\n",
    "    y = train[['week_1_res','week_2_res','week_3_res','week_4_res']]\n",
    "\n",
    "    X_test = test.drop(columns=['week_1_res','week_2_res','week_3_res','week_4_res'])\n",
    "    y_test= test[['week_1_res','week_2_res','week_3_res','week_4_res']]\n",
    "    \n",
    "\n",
    "    base_rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=2332))\n",
    "    base_rf.fit(X, y)\n",
    "\n",
    "    importances = np.array([est.feature_importances_ for est in base_rf.estimators_])\n",
    "\n",
    "    mean_importances = importances.mean(axis=0)\n",
    "\n",
    "\n",
    "    selected_mask = mean_importances >= 0.01\n",
    "    selected_features = X.columns[selected_mask].tolist()\n",
    "    X_selected = X[selected_features]\n",
    "\n",
    "    # Step 3: Hyperparameter tuning with RandomizedSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    rf = RandomForestRegressor(random_state=2332)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        random_state=2332\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_selected, y)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    model_final = MultiOutputRegressor(best_model)\n",
    "    model_final.fit(X_selected, y)\n",
    "    dump(model_final, f\"models_rf/rf_arima_sarima_model_{country}_{model_name}.joblib\")\n",
    "    test_aux = test.copy()\n",
    "    prediction_columns = [f\"prediction_{h+1}_weeks\" for h in range(4)]\n",
    "    preds = model_final.predict(X_test[selected_features])\n",
    "    test_aux[prediction_columns] = preds\n",
    "\n",
    "    # Evaluate predictions\n",
    "    test_aux = test_aux.dropna()\n",
    "    mae0, rmse0 = eval_metrics(test_aux[\"week_1_res\"], test_aux[\"prediction_1_weeks\"])\n",
    "    mae1, rmse1  = eval_metrics(test_aux[\"week_2_res\"], test_aux[\"prediction_2_weeks\"])\n",
    "    mae2, rmse2  = eval_metrics(test_aux[\"week_3_res\"], test_aux[\"prediction_3_weeks\"])\n",
    "    mae3, rmse3 = eval_metrics(test_aux[\"week_4_res\"], test_aux[\"prediction_4_weeks\"])\n",
    "\n",
    "    mape = pd.concat([\n",
    "    mape,\n",
    "    pd.DataFrame([\n",
    "        [country, model_name, \"1_week\", mae0, rmse0],\n",
    "        [country, model_name, \"2_week\", mae1, rmse1],\n",
    "        [country, model_name, \"3_week\", mae2, rmse2],\n",
    "        [country, model_name, \"4_week\", mae3, rmse3]\n",
    "    ], columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "], ignore_index=True)\n",
    "    return model_final, selected_features, test_aux,mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed98996",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ari = ari['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ili = ili['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6228119",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ili\n",
    "#problem DK PL AT HR GR IE LU LT NL NO MT\n",
    "\n",
    "['SI', 'EE', 'FR', 'RO', 'HU','LV', 'BE']\n",
    "['LU', 'LT', 'NL', 'CZ', 'NO', 'MT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2 = pd.DataFrame(columns=['location','model','prediction_window','mae','rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1206c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ili:\n",
    "    print(f'the country running is {i}')\n",
    "    loaded_model = joblib.load(f'models/sarima_model_{i}_ILI.joblib')\n",
    "    order_model = loaded_model.order\n",
    "    seasonal_order_model = loaded_model.seasonal_order\n",
    "    data = ili[ili['location']==i].copy()\n",
    "    train, test = train_data(ili,i, \"2023-10-15\")\n",
    "    mape_train, train2,res_1 = forecast_arima_sarima_model(train, train, mape_ili, loaded_model,order_model=order_model,seasonal_order_model=seasonal_order_model, model_name=\"ARIMA\", country=i)\n",
    "    mape_arima, test_predictions,res_2 = forecast_arima_sarima_model(train, test, mape_ili, loaded_model,order_model=order_model,seasonal_order_model=seasonal_order_model, model_name=\"ARIMA\", country=i)\n",
    "    train3= train2[[ 'year_week', 'relative_humidity_2m',\n",
    "       'temperature_2m_max', 'temperature_2m_min', 'covid',\n",
    "       'week_1_res', 'week_2_res', 'week_3_res',\n",
    "       'week_4_res']]\n",
    "    test_predictions_2 = test_predictions[[ 'year_week', 'relative_humidity_2m',\n",
    "       'temperature_2m_max', 'temperature_2m_min', 'covid',\n",
    "       'week_1_res', 'week_2_res', 'week_3_res',\n",
    "       'week_4_res']]\n",
    "    train_use = create_features(train3)\n",
    "    test_use = create_features(test_predictions_2)\n",
    "    print(f'process random forest for {i}')\n",
    "    model_final, selected_features, test_predictions_rf,mape = rf_variable_selection_and_hyperparam_tuning(train_use,test_use,country=i, model_name=\"ARIMA\")\n",
    "    test_predictions_values = test_predictions[['value', 'prediction_1_weeks', 'prediction_2_weeks',\n",
    "       'prediction_3_weeks', 'prediction_4_weeks']].copy()\n",
    "    res_prediction = test_predictions_rf[['prediction_1_weeks', 'prediction_2_weeks',\n",
    "            'prediction_3_weeks', 'prediction_4_weeks']].copy()\n",
    "    combined_predictions = test_predictions_values[['prediction_1_weeks', 'prediction_2_weeks',\n",
    "                                                      'prediction_3_weeks', 'prediction_4_weeks']] + \\\n",
    "    res_prediction[['prediction_1_weeks', 'prediction_2_weeks',\n",
    "                                             'prediction_3_weeks', 'prediction_4_weeks']]\n",
    "    combined_df = test_predictions_values[['value']].copy()\n",
    "    combined_df[[\"prediction_1_weeks\", \"prediction_2_weeks\",\n",
    "                  \"prediction_3_weeks\", \"prediction_4_weeks\"]] = combined_predictions\n",
    "    plot_train_test(train,combined_df,'mixed_test_ILI',i,'RF')\n",
    "    combined_df.to_csv(f'mixed_model_{i}_ILI.csv')\n",
    "   # Evaluate predictions\n",
    "    for h in range(4):\n",
    "       shifted = combined_df[\"value\"].shift(-h)\n",
    "       preds = combined_df[f\"prediction_{h+1}_weeks\"]\n",
    "       valid_idx = ~shifted.isna()\n",
    "       y_true = shifted[valid_idx]\n",
    "       y_pred = preds[valid_idx]\n",
    "       mae, rmse = eval_metrics(y_true, y_pred)\n",
    "       mape_ili2 = pd.concat([\n",
    "          mape_ili2,\n",
    "          pd.DataFrame([[i, 'rf/arima', f\"{h+1}_week\", mae, rmse]],\n",
    "                           columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "       ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7970a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2.to_csv('mape_mixed_ili2.csv',sep=';',decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3239e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2 = pd.read_csv('mape_mixed_ili2.csv',sep=';',decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ed3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_variable_selection_and_hyperparam_tuning(train,test,country =None, model_name=None, mape=None):\n",
    "    X = train.drop(columns=['week_1_res','week_2_res','week_3_res','week_4_res'])\n",
    "    y = train[['week_1_res','week_2_res','week_3_res','week_4_res']]\n",
    "\n",
    "    X_test = test.drop(columns=['week_1_res','week_2_res','week_3_res','week_4_res'])\n",
    "    y_test= test[['week_1_res','week_2_res','week_3_res','week_4_res']]\n",
    "    \n",
    "\n",
    "    base_model =MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', random_state=2332, n_jobs=-1))\n",
    "\n",
    "    base_model.fit(X, y)\n",
    "\n",
    "    importances = np.array([est.feature_importances_ for est in base_model.estimators_])\n",
    "\n",
    "    mean_importances = importances.mean(axis=0)\n",
    "\n",
    "\n",
    "    selected_mask = mean_importances >= 0.01\n",
    "    selected_features = X.columns[selected_mask].tolist()\n",
    "    X_selected = X[selected_features]\n",
    "\n",
    "    # Step 3: Hyperparameter tuning with RandomizedSearchCV\n",
    "    param_grid = {\n",
    "        \"estimator__n_estimators\": [50, 100, 200],\n",
    "        \"estimator__max_depth\": [3, 4, 5, 10],\n",
    "        \"estimator__learning_rate\": np.linspace(0.01, 0.1, 10),\n",
    "        \"estimator__subsample\": [0.8, 1.0],\n",
    "        \"estimator__colsample_bytree\": [0.7, 0.8, 1.0],       # feature subsampling\n",
    "        \"estimator__min_child_weight\": [1, 3, 5],              # min data needed in a child\n",
    "        \"estimator__gamma\": [0, 0.1, 0.3, 0.5],                # min loss reduction to split\n",
    "        \"estimator__reg_alpha\": np.linspace(0, 1, 5),          # L1 regularization\n",
    "        \"estimator__reg_lambda\": np.linspace(0, 1, 5),         # L2 regularization\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        random_state=2332\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_selected, y)\n",
    "\n",
    "    model_final = random_search.best_estimator_\n",
    "    model_final.fit(X_selected, y)\n",
    "    dump(model_final, f\"models_xgboost/xgboost_mixed_model_{country}_{model_name}.joblib\")\n",
    "    test_aux = test.copy()\n",
    "    prediction_columns = [f\"prediction_{h+1}_weeks\" for h in range(4)]\n",
    "    preds = model_final.predict(X_test[selected_features])\n",
    "    test_aux[prediction_columns] = preds\n",
    "\n",
    "    # Evaluate predictions\n",
    "    test_aux = test_aux.dropna()\n",
    "    mae0, rmse0 = eval_metrics(test_aux[\"week_1_res\"], test_aux[\"prediction_1_weeks\"])\n",
    "    mae1, rmse1  = eval_metrics(test_aux[\"week_2_res\"], test_aux[\"prediction_2_weeks\"])\n",
    "    mae2, rmse2  = eval_metrics(test_aux[\"week_3_res\"], test_aux[\"prediction_3_weeks\"])\n",
    "    mae3, rmse3 = eval_metrics(test_aux[\"week_4_res\"], test_aux[\"prediction_4_weeks\"])\n",
    "\n",
    "    mape = pd.concat([\n",
    "    mape,\n",
    "    pd.DataFrame([\n",
    "        [country, model_name, \"1_week\", mae0, rmse0],\n",
    "        [country, model_name, \"2_week\", mae1, rmse1],\n",
    "        [country, model_name, \"3_week\", mae2, rmse2],\n",
    "        [country, model_name, \"4_week\", mae3, rmse3]\n",
    "    ], columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "], ignore_index=True)\n",
    "    return model_final, selected_features, test_aux,mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_ari:\n",
    "    print(f'the country running is {i}')\n",
    "    loaded_model = joblib.load(f'models/sarima_model_{i}_ARI.joblib')\n",
    "    order_model = loaded_model.order\n",
    "    seasonal_order_model = loaded_model.seasonal_order\n",
    "    data = ari[ari['location']==i].copy()\n",
    "    train, test = train_data(ari,i, \"2023-10-15\")\n",
    "    mape_train, train2,res_1 = forecast_arima_sarima_model(train, train, mape_ili, loaded_model,order_model=order_model,seasonal_order_model=seasonal_order_model, model_name=\"ARIMA\", country=i)\n",
    "    mape_arima, test_predictions,res_2 = forecast_arima_sarima_model(train, test, mape_ili, loaded_model,order_model=order_model,seasonal_order_model=seasonal_order_model, model_name=\"ARIMA\", country=i)\n",
    "    train3= train2[[ 'year_week', 'relative_humidity_2m',\n",
    "       'temperature_2m_max', 'temperature_2m_min', 'covid',\n",
    "       'week_1_res', 'week_2_res', 'week_3_res',\n",
    "       'week_4_res']]\n",
    "    test_predictions_2 = test_predictions[[ 'year_week', 'relative_humidity_2m',\n",
    "       'temperature_2m_max', 'temperature_2m_min', 'covid',\n",
    "       'week_1_res', 'week_2_res', 'week_3_res',\n",
    "       'week_4_res']]\n",
    "    train_use = create_features(train3)\n",
    "    test_use = create_features(test_predictions_2)\n",
    "    print(f'process random forest for {i}')\n",
    "    model_final, selected_features, test_predictions_rf,mape = xgboost_variable_selection_and_hyperparam_tuning(train_use,test_use,country=i, model_name=\"ARIMA\")\n",
    "    test_predictions_values = test_predictions[['value', 'prediction_1_weeks', 'prediction_2_weeks',\n",
    "       'prediction_3_weeks', 'prediction_4_weeks']].copy()\n",
    "    res_prediction = test_predictions_rf[['prediction_1_weeks', 'prediction_2_weeks',\n",
    "            'prediction_3_weeks', 'prediction_4_weeks']].copy()\n",
    "    combined_predictions = test_predictions_values[['prediction_1_weeks', 'prediction_2_weeks',\n",
    "                                                      'prediction_3_weeks', 'prediction_4_weeks']] + \\\n",
    "    res_prediction[['prediction_1_weeks', 'prediction_2_weeks',\n",
    "                                             'prediction_3_weeks', 'prediction_4_weeks']]\n",
    "    combined_df = test_predictions_values[['value']].copy()\n",
    "    combined_df[[\"prediction_1_weeks\", \"prediction_2_weeks\",\n",
    "                  \"prediction_3_weeks\", \"prediction_4_weeks\"]] = combined_predictions\n",
    "    plot_train_test(train,combined_df,'mixed_xgboost_test_ARI',i,'RF')\n",
    "    combined_df.to_csv(f'mixed_xgboost_model_{i}_ARI.csv')\n",
    "   # Evaluate predictions\n",
    "    for h in range(4):\n",
    "       shifted = combined_df[\"value\"].shift(-h)\n",
    "       preds = combined_df[f\"prediction_{h+1}_weeks\"]\n",
    "       valid_idx = ~shifted.isna()\n",
    "       y_true = shifted[valid_idx]\n",
    "       y_pred = preds[valid_idx]\n",
    "       mae, rmse = eval_metrics(y_true, y_pred)\n",
    "       mape_ili2 = pd.concat([\n",
    "          mape_ili2,\n",
    "          pd.DataFrame([[i, 'rf/arima', f\"{h+1}_week\", mae, rmse]],\n",
    "                           columns=['location', 'model', 'prediction_window', 'mae', 'rmse'])\n",
    "       ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ili2.to_csv('mape_mixed_xgboost_ari2.csv',sep=';',decimal=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
